{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implementing Twitter_Sentiment_Analysis using BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('demo1.csv')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and usernames\n",
    "    text = re.sub(r'[@#\\w]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) #This line loads the set of stopwords \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Apply preprocessing to the 'Text' column\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'nature', 'inspired', 'exhibit', 'place', 'uncertain', 'home', 'slow', 'funny', 'new', 'powerful', 'day', 'down', 'solutions', 'breathtaking', 'Feeling', 'ecstatic', 'Stressed', 'back', 'book', 'Excited', 'setbacks', 'workload', 'scientific', 'sleep', 'words', 'art', 'outcome', 'traffic', \"n't\", 'project', 'dedication', 'remarkable', 'adrenaline', 'interview', 'surprise', 'irritated', '@', 'vacation', 'noise', 'learning', 'thrilling', 'I', 'ongoing', \"'m\", 'feel', 'exam', 'issues', 'tedious', 'tonight', 'my', 'to', 'hate', 'enthusiasm', 'heartfelt', 'see', 'determined', 'dull', 'delicious', 'good', 'exhilarated', 'they', 'video', 'positive', 'our', 'grateful', 'exhausted', 'adventure', 'week', 'username', 'long', 'all', 'party', 'intriguing', 'amazing', 'tranquility', 'enchanted', 'passionate', 'delays', 'stressed', 'overwhelmed', 'performance', 'current', 'civilization', 'are', 'team', 'intense', 'unexpected', 'energized', 'situation', 'happy', 'friends', 'after', 'interruptions', 'for', 'deadline', 'artifact', 'coding', 'about', 'Missing', 'disappointed', 'loud', 'their', \"'s\", 'blessed', 'with', 'beauty', 'Loving', 'reading', 'announce', 'thrilled', 'plot', 'frustrated', 'innovative', 'a', 'questions', 'sad', 'journey', 'today', 'overwhelming', 'accomplishments', 'love', 'tomorrow', 'sleepless', 'annoyed', 'by', 'lonely', 'constant', 'feedback', 'tasks', 'results', 'view', 'meal', 'same', 'her', 'offer', 'this', 'discovery', 'Python', 'strenuous', 'presentation', 'future', 'hobbies', 'NLP', 'mystery', 'moved', 'at', 'anxious', 'indifferent', 'painting', 'town', 'next', 'achievements', 'responsibilities', 'persistent', 'clever', '&', 'historical', 'notifications', 'Mondays', 'intrigued', 'message', 'work', 'job', 'novel', 'upcoming', 'not', 'start', 'achieve', 'event', 'touching', '!', 'of', 'discouraged', 'magical', 'delighted', ',', 'opportunity', 'music', 'progress', 'charm', 'captivated', 'goals', 'workout', 'pranks', 'nervous', 'in', 'impressed', 'night', 'success', 'story', 'silly', 'jokes', 'so', 'repetitive', 'because', 'routine', 'lack', 'monotonous', 'his', 'proud', 'ancient', 'Ca', 'family', 'complex', 'rush', 'fascinated', 'emotional', 'amused', 'feeling', 'exams', 'the', 'bored', 'am']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization and extracting unique words\n",
    "vocabulary = set()\n",
    "for text in df['Text']:\n",
    "    tokens = word_tokenize(text)\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "# Convert the set of unique words to a list\n",
    "vocabulary_list = list(vocabulary)\n",
    "\n",
    "# Print the vocabulary list\n",
    "print(vocabulary_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove usernames and stopwords\n",
    "def preprocess_text(text):\n",
    "    # Remove usernames\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Apply preprocessing to each text in the list\n",
    "processed_texts_with_labels = [(preprocess_text(text), label) for text, label in texts_with_labels]\n",
    "\n",
    "# Print the processed texts with labels\n",
    "for processed_text, label in processed_texts_with_labels:\n",
    "    print(processed_text, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets:\n",
      "I am happy because I am learning NLP\n",
      "I am happy @username\n",
      "I love coding in Python\n",
      "Excited for my vacation next week!\n",
      "Loving this new book I'm reading\n",
      "Can't wait to see my friends tomorrow!\n",
      "I feel so blessed today\n",
      "I am grateful for this opportunity\n",
      "I'm ecstatic about the new job offer\n",
      "Excited to start my new project\n",
      "I'm passionate about my hobbies\n",
      "I'm thrilled to announce our success\n",
      "I'm proud of my accomplishments\n",
      "I'm inspired by the beauty of nature\n",
      "I'm fascinated by this new discovery\n",
      "I'm determined to achieve my goals\n",
      "I'm intrigued by this mystery novel\n",
      "I'm captivated by this art exhibit\n",
      "I'm enchanted by this magical place\n",
      "I'm delighted by the surprise party\n",
      "I'm moved by this touching story\n",
      "I'm impressed by his dedication\n",
      "I'm energized after a good night's sleep\n",
      "I'm amused by this funny video\n",
      "I'm exhilarated by this thrilling adventure\n",
      "I'm captivated by her performance\n",
      "I'm fascinated by this ancient civilization\n",
      "I'm enchanted by the beauty of this place\n",
      "I'm delighted by the unexpected surprise\n",
      "I'm moved by his heartfelt words\n",
      "I'm impressed by her achievements\n",
      "I'm energized by the positive feedback\n",
      "I'm amused by their silly jokes\n",
      "I'm exhilarated by the adrenaline rush\n",
      "I'm captivated by the intriguing plot\n",
      "I'm fascinated by this scientific discovery\n",
      "I'm enchanted by the charm of this town\n",
      "I'm delighted by the amazing performance\n",
      "I'm moved by the powerful message\n",
      "I'm impressed by the innovative solutions\n",
      "I'm energized by the enthusiasm of the team\n",
      "I'm amused by their clever pranks\n",
      "I'm exhilarated by the breathtaking view\n",
      "I'm captivated by the beauty of this painting\n",
      "I'm fascinated by this historical artifact\n",
      "I'm enchanted by the tranquility of this place\n",
      "I'm delighted by the delicious meal\n",
      "I'm moved by the emotional performance\n",
      "I'm impressed by the remarkable achievements\n",
      "\n",
      "Negative Tweets:\n",
      "I am sad, I am not learning NLP\n",
      "I am sad & frustrated\n",
      "I hate Mondays, they are so dull\n",
      "Stressed about my upcoming exams\n",
      "Feeling lonely tonight\n",
      "I'm feeling frustrated with my work\n",
      "Feeling anxious about the presentation\n",
      "Missing my family back home\n",
      "Feeling disappointed with the results\n",
      "Feeling overwhelmed with all the tasks\n",
      "I'm feeling down today\n",
      "Feeling exhausted after a long day\n",
      "Feeling nervous about the interview\n",
      "Feeling discouraged by the setbacks\n",
      "Feeling annoyed by the constant noise\n",
      "Feeling bored with the routine\n",
      "Feeling indifferent about the outcome\n",
      "Feeling irritated by the traffic\n",
      "Feeling exhausted after the workout\n",
      "Feeling frustrated with the delays\n",
      "Feeling anxious about the future\n",
      "Feeling stressed about the deadline\n",
      "Feeling overwhelmed by the workload\n",
      "Feeling annoyed by the constant interruptions\n",
      "Feeling bored with the same routine\n",
      "Feeling irritated by the loud music\n",
      "Feeling exhausted after the long journey\n",
      "Feeling frustrated with the lack of progress\n",
      "Feeling anxious about the upcoming event\n",
      "Feeling stressed about the current situation\n",
      "Feeling overwhelmed by the responsibilities\n",
      "Feeling exhausted after the strenuous workout\n",
      "Feeling annoyed by the constant notifications\n",
      "Feeling bored with the monotonous routine\n",
      "Feeling irritated by the persistent noise\n",
      "Feeling exhausted after the sleepless night\n",
      "Feeling frustrated with the ongoing issues\n",
      "Feeling anxious about the uncertain future\n",
      "Feeling stressed about the upcoming exam\n",
      "Feeling overwhelmed by the complex situation\n",
      "Feeling exhausted after the long day at work\n",
      "Feeling annoyed by the repetitive questions\n",
      "Feeling bored with the tedious tasks\n",
      "Feeling irritated by the constant interruptions\n",
      "Feeling exhausted after the intense workout\n",
      "Feeling frustrated with the slow progress\n",
      "Feeling anxious about the upcoming deadline\n",
      "Feeling stressed about the uncertain future\n",
      "Feeling overwhelmed by the overwhelming tasks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter positive and negative tweets\n",
    "positive_tweets = df[df['Label'] == 1]['Text'].tolist()\n",
    "negative_tweets = df[df['Label'] == 0]['Text'].tolist()\n",
    "\n",
    "# Print positive and negative tweets corpus separately\n",
    "print(\"Positive Tweets:\")\n",
    "for tweet in positive_tweets:\n",
    "    print(tweet)\n",
    "\n",
    "print(\"\\nNegative Tweets:\")\n",
    "for tweet in negative_tweets:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Word Frequencies:\n",
      "wait: 1\n",
      "nature: 1\n",
      "inspired: 1\n",
      "exhibit: 1\n",
      "place: 3\n",
      "uncertain: 0\n",
      "home: 0\n",
      "slow: 0\n",
      "funny: 1\n",
      "new: 4\n",
      "powerful: 1\n",
      "day: 0\n",
      "down: 0\n",
      "solutions: 1\n",
      "breathtaking: 1\n",
      "Feeling: 0\n",
      "ecstatic: 1\n",
      "Stressed: 0\n",
      "back: 0\n",
      "book: 1\n",
      "Excited: 2\n",
      "setbacks: 0\n",
      "workload: 0\n",
      "scientific: 1\n",
      "sleep: 1\n",
      "words: 1\n",
      "art: 1\n",
      "outcome: 0\n",
      "traffic: 0\n",
      "n't: 1\n",
      "project: 1\n",
      "dedication: 1\n",
      "remarkable: 1\n",
      "adrenaline: 1\n",
      "interview: 0\n",
      "surprise: 2\n",
      "irritated: 0\n",
      "@: 1\n",
      "vacation: 1\n",
      "noise: 0\n",
      "learning: 1\n",
      "thrilling: 1\n",
      "I: 47\n",
      "ongoing: 0\n",
      "'m: 41\n",
      "feel: 1\n",
      "exam: 0\n",
      "issues: 0\n",
      "tedious: 0\n",
      "tonight: 0\n",
      "my: 6\n",
      "to: 4\n",
      "hate: 0\n",
      "enthusiasm: 1\n",
      "heartfelt: 1\n",
      "see: 1\n",
      "determined: 1\n",
      "dull: 0\n",
      "delicious: 1\n",
      "good: 1\n",
      "exhilarated: 3\n",
      "they: 0\n",
      "video: 1\n",
      "positive: 1\n",
      "our: 1\n",
      "grateful: 1\n",
      "exhausted: 0\n",
      "adventure: 1\n",
      "week: 1\n",
      "username: 1\n",
      "long: 0\n",
      "all: 0\n",
      "party: 1\n",
      "intriguing: 1\n",
      "amazing: 1\n",
      "tranquility: 1\n",
      "enchanted: 4\n",
      "passionate: 1\n",
      "delays: 0\n",
      "stressed: 0\n",
      "overwhelmed: 0\n",
      "performance: 3\n",
      "current: 0\n",
      "civilization: 1\n",
      "are: 0\n",
      "team: 1\n",
      "intense: 0\n",
      "unexpected: 1\n",
      "energized: 3\n",
      "situation: 0\n",
      "happy: 2\n",
      "friends: 1\n",
      "after: 1\n",
      "interruptions: 0\n",
      "for: 2\n",
      "deadline: 0\n",
      "artifact: 1\n",
      "coding: 1\n",
      "about: 2\n",
      "Missing: 0\n",
      "disappointed: 0\n",
      "loud: 0\n",
      "their: 2\n",
      "'s: 1\n",
      "blessed: 1\n",
      "with: 0\n",
      "beauty: 3\n",
      "Loving: 1\n",
      "reading: 1\n",
      "announce: 1\n",
      "thrilled: 1\n",
      "plot: 1\n",
      "frustrated: 0\n",
      "innovative: 1\n",
      "a: 1\n",
      "questions: 0\n",
      "sad: 0\n",
      "journey: 0\n",
      "today: 1\n",
      "overwhelming: 0\n",
      "accomplishments: 1\n",
      "love: 1\n",
      "tomorrow: 1\n",
      "sleepless: 0\n",
      "annoyed: 0\n",
      "by: 34\n",
      "lonely: 0\n",
      "constant: 0\n",
      "feedback: 1\n",
      "tasks: 0\n",
      "results: 0\n",
      "view: 1\n",
      "meal: 1\n",
      "same: 0\n",
      "her: 2\n",
      "offer: 1\n",
      "this: 16\n",
      "discovery: 2\n",
      "Python: 1\n",
      "strenuous: 0\n",
      "presentation: 0\n",
      "future: 0\n",
      "hobbies: 1\n",
      "NLP: 1\n",
      "mystery: 1\n",
      "moved: 4\n",
      "at: 0\n",
      "anxious: 0\n",
      "indifferent: 0\n",
      "painting: 1\n",
      "town: 1\n",
      "next: 1\n",
      "achievements: 2\n",
      "responsibilities: 0\n",
      "persistent: 0\n",
      "clever: 1\n",
      "&: 0\n",
      "historical: 1\n",
      "notifications: 0\n",
      "Mondays: 0\n",
      "intrigued: 1\n",
      "message: 1\n",
      "work: 0\n",
      "job: 1\n",
      "novel: 1\n",
      "upcoming: 0\n",
      "not: 0\n",
      "start: 1\n",
      "achieve: 1\n",
      "event: 0\n",
      "touching: 1\n",
      "!: 2\n",
      "of: 7\n",
      "discouraged: 0\n",
      "magical: 1\n",
      "delighted: 4\n",
      ",: 0\n",
      "opportunity: 1\n",
      "music: 0\n",
      "progress: 0\n",
      "charm: 1\n",
      "captivated: 4\n",
      "goals: 1\n",
      "workout: 0\n",
      "pranks: 1\n",
      "nervous: 0\n",
      "in: 1\n",
      "impressed: 4\n",
      "night: 1\n",
      "success: 1\n",
      "story: 1\n",
      "silly: 1\n",
      "jokes: 1\n",
      "so: 1\n",
      "repetitive: 0\n",
      "because: 1\n",
      "routine: 0\n",
      "lack: 0\n",
      "monotonous: 0\n",
      "his: 2\n",
      "proud: 1\n",
      "ancient: 1\n",
      "Ca: 1\n",
      "family: 0\n",
      "complex: 0\n",
      "rush: 1\n",
      "fascinated: 4\n",
      "emotional: 1\n",
      "amused: 3\n",
      "feeling: 0\n",
      "exams: 0\n",
      "the: 20\n",
      "bored: 0\n",
      "am: 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store word frequencies\n",
    "positive_freq = {word: 0 for word in vocabulary}\n",
    "negative_freq = {word: 0 for word in vocabulary}\n",
    "\n",
    "# Calculate positive word frequencies\n",
    "for tweet in positive_tweets:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            positive_freq[token] += 1\n",
    "\n",
    "# Calculate negative word frequencies\n",
    "for tweet in negative_tweets:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            negative_freq[token] += 1\n",
    "\n",
    "# Print positive and negative word frequencies\n",
    "print(\"Positive Word Frequencies:\")\n",
    "for word, freq in positive_freq.items():\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Word Frequencies:\n",
      "changing: 0\n",
      "conversation: 0\n",
      "friend: 0\n",
      "park: 0\n",
      "so: 1\n",
      "ones: 1\n",
      "book: 0\n",
      "semester: 0\n",
      "favorite: 3\n",
      "attend: 0\n",
      "chapter: 0\n",
      "kids: 0\n",
      "being: 1\n",
      "month: 0\n",
      "support: 0\n",
      "rainy: 1\n",
      "who: 2\n",
      "concert: 0\n",
      "mistakes: 1\n",
      "that: 3\n",
      "finished: 0\n",
      "exercise: 0\n",
      "health: 0\n",
      "my: 13\n",
      "sticking: 0\n",
      "right: 1\n",
      "recipe: 0\n",
      "Not: 1\n",
      "grateful: 0\n",
      "job: 1\n",
      "game: 1\n",
      "tea: 0\n",
      "meals: 1\n",
      "well-being: 0\n",
      "pursue: 0\n",
      "dreams: 0\n",
      "graduation: 0\n",
      "overcoming: 0\n",
      "with: 4\n",
      "while: 1\n",
      "home-cooked: 1\n",
      "for: 0\n",
      "Grateful: 0\n",
      "trip: 1\n",
      "little: 0\n",
      "gatherings: 0\n",
      "Enjoying: 0\n",
      "dull: 1\n",
      "car: 1\n",
      "'m: 0\n",
      "start: 0\n",
      "simpler: 1\n",
      "summer: 0\n",
      "popcorn: 0\n",
      "had: 0\n",
      "road: 0\n",
      "interview: 1\n",
      "week: 0\n",
      "exhausted: 1\n",
      "travel: 4\n",
      "lonely: 3\n",
      "puppy: 0\n",
      "leaves: 0\n",
      "sad: 2\n",
      "confident: 1\n",
      "completed: 0\n",
      "without: 2\n",
      "coffee: 1\n",
      "an: 5\n",
      "grandma: 1\n",
      "team: 1\n",
      "Bored: 1\n",
      "moments: 0\n",
      "workout: 0\n",
      "because: 0\n",
      "next: 0\n",
      "birthday: 0\n",
      "turn: 2\n",
      "restaurant: 0\n",
      "meal: 0\n",
      "fix: 1\n",
      "exams: 1\n",
      "Feeling: 18\n",
      "cozy: 0\n",
      "event: 1\n",
      "Excited: 0\n",
      "world: 1\n",
      "bad: 2\n",
      "happy: 0\n",
      "learn: 0\n",
      "finishing: 0\n",
      "listening: 0\n",
      "speech: 0\n",
      "back: 3\n",
      "family: 0\n",
      "exam: 1\n",
      "the: 5\n",
      "myself: 0\n",
      "disappointed: 1\n",
      "motivational: 0\n",
      "proud: 0\n",
      "frustrated: 1\n",
      "delicious: 0\n",
      "see: 0\n",
      "upcoming: 6\n",
      "slow: 1\n",
      "beauty: 0\n",
      "nostalgic: 0\n",
      "and: 3\n",
      "productive: 1\n",
      "Celebrating: 0\n",
      "hobby: 0\n",
      "loved: 1\n",
      "make: 1\n",
      "time: 3\n",
      "adventures: 1\n",
      "Stressed: 1\n",
      "they: 1\n",
      "vacation: 0\n",
      "snacks: 0\n",
      "accomplished: 0\n",
      "routine: 0\n",
      "all: 2\n",
      "this: 2\n",
      "strong: 0\n",
      "mountains: 0\n",
      "nervous: 1\n",
      "opportunities: 0\n",
      "be: 2\n",
      "life: 1\n",
      "old: 0\n",
      "NLP: 1\n",
      "connection: 1\n",
      "promoted: 0\n",
      "got: 1\n",
      "at: 2\n",
      "could: 8\n",
      "show: 1\n",
      "energized: 0\n",
      "day: 2\n",
      "ride: 1\n",
      "forward: 1\n",
      "'s: 1\n",
      "motivated: 0\n",
      "presentation: 2\n",
      "tomorrow: 2\n",
      "different: 1\n",
      "a: 3\n",
      "when: 1\n",
      "weekend: 0\n",
      "pampering: 0\n",
      "places: 1\n",
      "spa: 0\n",
      "joy: 0\n",
      "social: 1\n",
      "choices: 1\n",
      "studying: 1\n",
      "internet: 1\n",
      "siblings: 1\n",
      "lately: 1\n",
      "watching: 0\n",
      "getaway: 0\n",
      "home: 1\n",
      "lost: 1\n",
      "not: 1\n",
      "exotic: 1\n",
      "news: 1\n",
      "anxious: 5\n",
      "fall: 0\n",
      "by: 1\n",
      "canceled: 1\n",
      "down: 1\n",
      "relaxing: 0\n",
      "skill: 0\n",
      "!: 0\n",
      "Just: 0\n",
      "childhood: 1\n",
      "challenge: 0\n",
      "in: 2\n",
      "release: 0\n",
      "closed: 1\n",
      "coding: 0\n",
      "after: 2\n",
      "carefree: 1\n",
      "long: 2\n",
      "school: 0\n",
      "hate: 1\n",
      "quiet: 0\n",
      "cup: 0\n",
      "now: 1\n",
      "today: 0\n",
      "holidays: 0\n",
      "band: 0\n",
      "try: 0\n",
      "nature: 0\n",
      "inspired: 0\n",
      "picnic: 0\n",
      "friends: 2\n",
      "season: 0\n",
      "I: 12\n",
      "do: 1\n",
      "n't: 0\n",
      "shop: 1\n",
      "deadlines: 1\n",
      "Mondays: 1\n",
      "supportive: 0\n",
      "spend: 0\n",
      "new: 0\n",
      "Python: 0\n",
      "about: 7\n",
      "evening: 0\n",
      "tonight: 1\n",
      "learning: 1\n",
      "away: 2\n",
      "of: 1\n",
      "project: 0\n",
      "movie: 1\n",
      "great: 0\n",
      "language: 0\n",
      "reading: 0\n",
      "beach: 0\n",
      "work: 3\n",
      "are: 1\n",
      "Bummed: 3\n",
      "songs: 0\n",
      "am: 3\n",
      "year: 0\n",
      "far: 2\n",
      "weather: 0\n",
      "challenging: 0\n",
      "Ca: 0\n",
      "feeling: 1\n",
      "Looking: 0\n",
      "dinner: 0\n",
      "was: 1\n",
      "Missing: 6\n",
      "achieve: 0\n",
      "situations: 1\n",
      "bring: 0\n",
      "days: 1\n",
      "responsibilities: 1\n",
      ",: 2\n",
      "good: 0\n",
      "to: 3\n",
      "abroad: 1\n",
      "reunion: 0\n",
      "more: 2\n",
      "holiday: 0\n",
      "wait: 0\n",
      "night: 0\n",
      "bored: 2\n",
      "looking: 1\n",
      "overwhelmed: 3\n",
      "love: 0\n",
      "adopted: 0\n",
      "nothing: 1\n",
      "during: 2\n",
      "goals: 0\n",
      "pet: 1\n",
      "live: 2\n",
      "homesick: 1\n",
      "Loving: 0\n",
      "Wishing: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNegative Word Frequencies:\")\n",
    "for word, freq in negative_freq.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Vector: [1, 25, 32]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def encode_tweet(tweet, positive_freq, negative_freq):\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # Initialize variables to store frequencies\n",
    "    positive_sum = 0\n",
    "    negative_sum = 0\n",
    "\n",
    "    # Calculate the sum of positive and negative frequencies\n",
    "    for token in tokens:\n",
    "        if token in positive_freq:\n",
    "            positive_sum += positive_freq[token]\n",
    "        if token in negative_freq:\n",
    "            negative_sum += negative_freq[token]\n",
    "\n",
    "    # Encode the tweet into the specified format\n",
    "    encoded_vector = [1, positive_sum, negative_sum]\n",
    "    return encoded_vector\n",
    "\n",
    "# Example usage:\n",
    "tweet = \"I am happy because I am learning NLP\"\n",
    "encoded_vector = encode_tweet(tweet, positive_freq, negative_freq)\n",
    "print(\"Encoded Vector:\", encoded_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X:\n",
      "[[ 1. 25. 32.]\n",
      " [ 1. 27. 37.]\n",
      " [ 1. 17. 14.]\n",
      " [ 1. 14. 20.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def encode_tweet(tweet, positive_freq, negative_freq):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    positive_sum = sum(positive_freq.get(token, 0) for token in tokens)\n",
    "    negative_sum = sum(negative_freq.get(token, 0) for token in tokens)\n",
    "    return [1, positive_sum, negative_sum]\n",
    "\n",
    "\n",
    "# Sample tweets\n",
    "tweets = [\n",
    "    \"I am happy because I am learning NLP\",\n",
    "    \"I am sad, I am not learning NLP\",\n",
    "    \"I love coding in Python\",\n",
    "    \"I hate Mondays, they are so dull\"\n",
    "]\n",
    "\n",
    "# Initialize matrix X\n",
    "X = np.zeros((len(tweets), 3))\n",
    "\n",
    "# Extract features for each tweet and populate the matrix X\n",
    "for idx, tweet in enumerate(tweets):\n",
    "    encoded_vector = encode_tweet(tweet, positive_freq, negative_freq)\n",
    "    X[idx, :] = encoded_vector\n",
    "\n",
    "print(\"Matrix X:\")\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Theta: [-0.25119726  3.89650908 -2.93435979]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "tweets = [\n",
    "    \"I am happy because I am learning NLP\",\n",
    "    \"I am sad, I am not learning NLP\",\n",
    "    \"I love coding in Python\",\n",
    "    \"I hate Mondays, they are so dull\"\n",
    "]\n",
    "\n",
    "labels = np.array([1, 0, 1, 0])  # 1 for positive, 0 for negative\n",
    "\n",
    "# Encode tweets\n",
    "def encode_tweet(tweet, positive_freq, negative_freq):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    positive_sum = sum(positive_freq.get(token, 0) for token in tokens)\n",
    "    negative_sum = sum(negative_freq.get(token, 0) for token in tokens)\n",
    "    return [1, positive_sum, negative_sum]\n",
    "\n",
    "X = np.array([encode_tweet(tweet, positive_freq, negative_freq) for tweet in tweets])\n",
    "\n",
    "# Initialize parameters (theta)\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function.\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    epsilon = 1e-5  # to avoid log(0)\n",
    "    cost = -1/m * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "    return cost\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for _ in range(iterations):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        gradient = X.T.dot(h - y) / m\n",
    "        theta -= alpha * gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        costs.append(cost)\n",
    "    return theta, costs\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimized, costs = gradient_descent(X, labels, theta, alpha, iterations)\n",
    "\n",
    "# Make predictions on the training data\n",
    "predictions = np.round(sigmoid(X.dot(theta_optimized)))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(\"Optimized Theta:\", theta_optimized)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I am happy because I am learning NLP - Sentiment: positive\n",
      "Tweet: I am sad, I am not learning NLP - Sentiment: negative\n",
      "Tweet: I love coding in Python - Sentiment: positive\n",
      "Tweet: I hate Mondays, they are so dull - Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Define a threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Make predictions based on the threshold\n",
    "probabilities = sigmoid(X.dot(theta_optimized))\n",
    "predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Print predictions\n",
    "for tweet, pred in zip(tweets, predictions):\n",
    "    sentiment = \"positive\" if pred == 1 else \"negative\"\n",
    "    print(f\"Tweet: {tweet} - Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.round(sigmoid(X.dot(theta_optimized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy manually\n",
    "correct_predictions = np.sum(predictions == labels)\n",
    "total_samples = len(labels)\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
